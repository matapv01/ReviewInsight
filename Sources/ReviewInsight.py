# -*- coding: utf-8 -*-
"""Copy of ReviewInsight to review.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14utSkhyJym8R6goo61S59cHDetg4OYft

# 1. Cài đặt các thư viện cần thiết
"""

!pip install pandas
!pip install sentence-transformers
!pip install numpy
!pip install matplotlib
!pip install seaborn

"""# Mount drive"""

from google.colab import drive
drive.mount('/content/drive')

"""#2. Đọc dữ liệu từ file CSV và hiển thị"""

import pandas as pd

# Đọc dữ liệu từ file CSV
file_path = "cleaned_amazon_reviews.csv"
df = pd.read_csv(file_path)

# Xem qua dữ liệu
print(df.head())

"""# 3. Tiền xử lý dữ liệu"""

import re

# Hàm tiền xử lý văn bản
def preprocess_text(text):
    text = text.lower()  # Chuyển thành chữ thường
    text = re.sub(r'[^\w\s]', '', text)  # Xóa dấu câu
    text = text.strip()  # Xóa khoảng trắng thừa
    return text

# Áp dụng hàm tiền xử lý
df['Đánh giá'] = df['Đánh giá'].apply(preprocess_text)

"""# 4. Tạo Embeddings với sentenceBERT"""

from sentence_transformers import SentenceTransformer

# Khởi tạo mô hình sentence-BERT
model = SentenceTransformer('all-MiniLM-L6-v2')

# Tạo embeddings cho các đánh giá
embeddings = model.encode(df['Đánh giá'].tolist())

"""# 5. Phân Cụm Dữ Liệu"""

from sklearn.cluster import KMeans

# Số lượng cụm (clusters) bạn muốn
num_clusters = 5

# Khởi tạo và huấn luyện mô hình K-Means
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(embeddings)

# Thêm nhãn cụm vào DataFrame
df['Cluster'] = kmeans.labels_

# Xem kết quả phân cụm
print(df.head())

"""# 6. Phân Tích Kết Quả"""

import matplotlib.pyplot as plt

# Phân tích số lượng đánh giá trong mỗi cụm
cluster_counts = df['Cluster'].value_counts()
print(cluster_counts)

# Vẽ biểu đồ số lượng đánh giá trong mỗi cụm
plt.bar(cluster_counts.index, cluster_counts.values)
plt.xlabel('Cluster')
plt.ylabel('Số lượng đánh giá')
plt.title('Số lượng đánh giá trong mỗi cụm')
plt.show()

"""# 7. Trực Quan Hóa Dữ Liệu"""

from sklearn.decomposition import PCA
import seaborn as sns

# Giảm chiều dữ liệu xuống 2D
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)

# Tạo DataFrame cho t-SNE
tsne_df = pd.DataFrame(reduced_embeddings, columns=['PC1', 'PC2'])
tsne_df['Cluster'] = df['Cluster']

# Vẽ biểu đồ phân cụm
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=tsne_df, palette='viridis')
plt.title('Trực quan hóa phân cụm')
plt.show()

"""# 8. Cài đặt thư viện transformers và huggingface_hub"""

!pip install transformers torch huggingface_hub

import pandas as pd

# df là DataFrame chứa các đánh giá và nhãn cụm
# Gộp tất cả đánh giá của mỗi cụm thành một văn bản duy nhất
cluster_texts = df.groupby('Cluster')['Đánh giá'].apply(lambda x: ' '.join(x)).reset_index()
cluster_texts.columns = ['Cluster', 'Text']

# Kiểm tra kết quả
print(cluster_texts.head())

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

access_token = ""  # Thay bằng token của bạn
model_name = "meta-llama/Meta-Llama-3.1-8B"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token)

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    use_auth_token=access_token,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # Quan trọng cho việc quản lý bộ nhớ với model lớn
    load_in_8bit=False,  # Đảm bảo không load ở 8-bit
    load_in_4bit=False  # Đảm bảo không load ở 4-bit
)

# Kiểm tra pad_token_id
print(f"Pad token ID: {tokenizer.pad_token_id}")
print(f"EOS token ID: {tokenizer.eos_token_id}")

# Nếu pad_token_id không được thiết lập thì thiết lập nó
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

# Kiểm tra pad_token_id
print(f"Pad token ID: {tokenizer.pad_token_id}")
print(f"EOS token ID: {tokenizer.eos_token_id}")

from tqdm import tqdm

# Hàm tóm tắt văn bản
def generate_summary(text, max_new_tokens=150):
    # Tokenize input text
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

    # Lấy attention mask và pad token id
    attention_mask = inputs.get('attention_mask', None)
    pad_token_id = tokenizer.pad_token_id

    # Generate summary
    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            attention_mask=attention_mask,  # Thêm attention_mask
            pad_token_id=pad_token_id,      # Đặt pad_token_id
            max_new_tokens=max_new_tokens,   # Sử dụng max_new_tokens để điều chỉnh độ dài đầu ra
            num_beams=4,
            early_stopping=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Áp dụng mô hình để tạo ra tóm tắt cho mỗi cụm với thanh tiến trình
tqdm.pandas(desc="Đang tóm tắt các cụm...")
cluster_texts['Summary'] = cluster_texts['Text'].progress_apply(generate_summary)

# Xem kết quả
print(cluster_texts.head())

# Chọn hai cột cần xuất
cluster_texts[['Cluster', 'Summary']].to_csv('ket_qua_cum.csv', index=False)